{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2dbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmartRedis Library@21-23-48:WARNING: Environment variable SR_LOG_FILE is not set. Defaulting to stdout\n",
      "SmartRedis Library@21-23-48:WARNING: Environment variable SR_LOG_LEVEL is not set. Defaulting to INFO\n",
      "['tensorflow', 'torch', 'onnxruntime']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from PyFoam.RunDictionary.ParsedParameterFile import ParsedParameterFile\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from smartsim import Experiment\n",
    "from smartredis import Client\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, layer_width, input_size, output_size, activation_fn):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, layer_width))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(layer_width, layer_width))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        layers.append(nn.Linear(layer_width, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "def sort_tensors_by_names(tensors, tensor_names):\n",
    "    # Pair each tensor with its name and sort by the name\n",
    "    pairs = sorted(zip(tensor_names, tensors))\n",
    "\n",
    "    # Extract the sorted tensors\n",
    "    tensor_names_sorted, tensors_sorted = zip(*pairs)\n",
    "\n",
    "    # Convert back to list if needed\n",
    "    tensor_names_sorted = list(tensor_names_sorted)\n",
    "    tensors_sorted = list(tensors_sorted)\n",
    "\n",
    "    return tensors_sorted, tensor_names_sorted\n",
    "\n",
    "def visualization_points(n_points):\n",
    "\n",
    "    domain_min = [-3, -3, 0]\n",
    "    domain_max = [3, 3, 0]\n",
    "    radius = 1\n",
    "\n",
    "    # Generate grid of points\n",
    "    x = np.linspace(domain_min[0], domain_max[0], n_points)\n",
    "    y = np.linspace(domain_min[1], domain_max[1], n_points)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid_points = np.column_stack((xx.ravel(), yy.ravel(), np.zeros(n_points**2)))\n",
    "\n",
    "    # Filter out points within the circle\n",
    "    norm = np.linalg.norm(grid_points[:, :2], axis=1)\n",
    "    visualization_points = grid_points[norm > radius]\n",
    "\n",
    "    return visualization_points\n",
    "\n",
    "\n",
    "exp = Experiment(\"mesh-motion\", launcher=\"local\")\n",
    "\n",
    "db = exp.create_database(port=8000,       # database port\n",
    "                         interface=\"lo\")  # network interface to use\n",
    "exp.start(db)\n",
    "\n",
    "# Connect the python client to the smartredis database\n",
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "num_mpi_ranks = 4\n",
    "\n",
    "of_rs = exp.create_run_settings(exe=\"moveDynamicMesh\", exe_args=\"-case spinningDisk -parallel\", \n",
    "                                run_command=\"mpirun\", \n",
    "                                run_args={\"np\": f\"{num_mpi_ranks}\"})\n",
    "\n",
    "of_model = exp.create_model(name=\"of_model\", run_settings=of_rs)\n",
    "\n",
    "# List installed ML backends\n",
    "from smartsim._core.utils.helpers import installed_redisai_backends\n",
    "print(installed_redisai_backends())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7838a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Run the experiment\n",
    "    exp.start(of_model, block=False)\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    # Make sure all datasets are avaialble in the smartredis database.\n",
    "    local_time_index = 1\n",
    "    while True:\n",
    "\n",
    "        if client.poll_key(\"end_time_index\", 10, 1000):\n",
    "            print (\"End time reached.\")\n",
    "            break\n",
    "        \n",
    "        print (f\"Time step {local_time_index}\")\n",
    "          \n",
    "        # Fetch datasets from SmartRedis\n",
    " \n",
    "        # - Poll until the points datasets are written by OpenFOAM\n",
    "        # print (f\"dataset_list_length {dataset_list_length}\") # Debug info\n",
    "        points_updated = client.poll_list_length(\"pointsDatasetList\", \n",
    "                                                 num_mpi_ranks, 10, 1000);\n",
    "        if (not points_updated):\n",
    "            raise ValueError(\"Points dataset list not updated.\")\n",
    "            \n",
    "        # - Poll until the displacements datasets are written by OpenFOAM\n",
    "        # print (f\"dataset_list_length {dataset_list_length}\") # Debug info\n",
    "        displacements_updated = client.poll_list_length(\"displacementsDatasetList\", \n",
    "                                                         num_mpi_ranks, 10, 1000);\n",
    "        if (not displacements_updated):\n",
    "            raise ValueError(\"Displacements dataset list not updated.\")\n",
    "            \n",
    "        # - Get the points and displacements datasets from SmartRedis\n",
    "        points_datasets = client.get_datasets_from_list(\"pointsDatasetList\")  \n",
    "        displacements_datasets = client.get_datasets_from_list(\"displacementsDatasetList\")\n",
    "        \n",
    "        print(f\"Lenght of the dataset list is {len(points_datasets)}\")\n",
    "      \n",
    "        # - Use only the datasets from this time step\n",
    "        #points_datasets = points_datasets[-num_mpi_ranks:]        \n",
    "        #displacements_datasets = displacements_datasets[-num_mpi_ranks:]\n",
    "        \n",
    "        # - Agglomerate all tensors from points and displacements datasets: \n",
    "        #   sort tensors by their names to ensure matching patches of same MPI ranks\n",
    "        points = []\n",
    "        points_names = []\n",
    "        displacements = []\n",
    "        displacements_names = []\n",
    "        \n",
    "        for points_dset, displs_dset in zip(points_datasets, displacements_datasets):\n",
    "            points_tensor_names = points_dset.get_tensor_names()\n",
    "            displs_tensor_names = displs_dset.get_tensor_names()\n",
    "            for points_name,displs_name in zip(points_tensor_names,displs_tensor_names):\n",
    "                patch_points = points_dset.get_tensor(points_name)\n",
    "                points.append(patch_points)\n",
    "                points_names.append(points_name)\n",
    "\n",
    "                patch_displs = displs_dset.get_tensor(displs_name)\n",
    "                displacements.append(patch_displs)\n",
    "                displacements_names.append(displs_name)\n",
    "                \n",
    "        points, points_names = sort_tensors_by_names(points, points_names)\n",
    "        displacements, displacements_names = sort_tensors_by_names(displacements, displacements_names)\n",
    "        \n",
    "        # - Reshape points and displacements into [N_POINTS,SPATIAL_DIMENSION] tensors\n",
    "        #   This basically agglomerates data from OpenFOAM boundary patches into a list\n",
    "        #   of boundary points (unstructured) and a list of respective point displacements. \n",
    "        points = torch.from_numpy(np.vstack(points))\n",
    "        displacements = torch.from_numpy(np.vstack(displacements))\n",
    "        \n",
    "        # Visualize domain points and displacements     \n",
    "        points_np = points.detach().numpy()\n",
    "        displ_np = displacements.detach().numpy()\n",
    "\n",
    "        plt.axis('equal')\n",
    "        plt.title(f\"Boundary points and displacements at time step {local_time_index}\")\n",
    "        plt.scatter(points_np[:, 0], points_np[:, 1], color='blue', s=5)\n",
    "        plt.quiver(points_np[:, 0], points_np[:, 1], \n",
    "                   displ_np[:, 0], displ_np[:, 1], color='green')        \n",
    "\n",
    "        \n",
    "        # TODO(TM): hardcoded x,y coordinates, make the OF client store polymesh::solutionD\n",
    "        #           and use solutionD non-zero values for sampling vector coordinates. \n",
    "        points = points[:, :2]\n",
    "        displacements = displacements[:, :2]\n",
    "\n",
    "        # Split training and validation data\n",
    "        points_train, points_val, displ_train, displ_val = train_test_split(points, displacements, \n",
    "                                                                            test_size=0.2, random_state=42)\n",
    "\n",
    "        # Fit the scaler on the training data\n",
    "        target_scaler = MinMaxScaler().fit(displ_train.numpy()) \n",
    "        \n",
    "        # Transform the training and validation data\n",
    "        displ_train = torch.from_numpy(target_scaler.transform(displ_train.numpy()))\n",
    "        displ_val = torch.from_numpy(target_scaler.transform(displ_val.numpy()))\n",
    "\n",
    "        # Initialize the model\n",
    "        model = MLP(num_layers=5, layer_width=20, input_size=2, output_size=2, activation_fn=torch.nn.Tanh())\n",
    "        model.train()\n",
    "\n",
    "        # PYTORCH Training Loop\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        loss_func = nn.MSELoss()\n",
    "        epochs = 500\n",
    "        mean_mag_displ = torch.mean(torch.norm(displ_train, dim=1))\n",
    "        for epoch in range(epochs):\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on the training data\n",
    "            displ_pred = model(points_train)\n",
    "\n",
    "            # Compute loss on the training data\n",
    "            loss_train = loss_func(displ_pred, displ_train)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Forward pass on the validation data, with torch.no_grad() for efficiency\n",
    "            with torch.no_grad():\n",
    "                displ_pred_val = model(points_val)\n",
    "                loss_val = loss_func(displ_pred_val, displ_val)\n",
    "                rmse_loss_val = torch.sqrt(loss_val)\n",
    "                # Uncomment to log training progress\n",
    "#                 print (f'Validation loss RMSE : {rmse_loss_val}, mean displacement magnitude {mean_mag_displ}')\n",
    "#                 print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss_train.item()}, Validation Loss: {loss_val.item()}')\n",
    "    \n",
    "\n",
    "        \n",
    "        # Visualize predicted displacements \n",
    "        plt.axis('equal')\n",
    "        displ_pred = model.forward(points)\n",
    "        print (f\"Shape of forward points {points.shape}\")\n",
    "        # Inverse transform the predictions\n",
    "        displ_pred_np = torch.from_numpy(target_scaler.inverse_transform(displ_pred.detach().numpy()))\n",
    "        plt.quiver(points_np[:, 0], points_np[:, 1], \n",
    "                   displ_pred_np[:, 0], displ_pred_np[:, 1], color='red', width=0.002)  \n",
    "        plt.figure()\n",
    "\n",
    "        # Store the model into SmartRedis\n",
    "        model.eval()\n",
    "        # Prepare a sample input\n",
    "        example_forward_input = torch.rand(2)\n",
    "        # Convert the PyTorch model to TorchScript\n",
    "        model_script = torch.jit.trace(model, example_forward_input)\n",
    "        # Save the TorchScript model to a buffer\n",
    "        model_buffer = io.BytesIO()\n",
    "        torch.jit.save(model_script, model_buffer)\n",
    "        # Set the model in the SmartRedis database\n",
    "        print(\"Saving model MLP\")\n",
    "        client.set_model(\"MLP\", model_buffer.getvalue(), \"TORCH\", \"CPU\")\n",
    "\n",
    "        # Update the model in smartredis\n",
    "        client.put_tensor(\"model_updated\", np.array([0.]))\n",
    "\n",
    "        # Delete dataset lists for the next time step\n",
    "        client.delete_list(\"pointsDatasetList\")\n",
    "        client.delete_list(\"displacementsDatasetList\")\n",
    "        \n",
    "        # Update time index\n",
    "        local_time_index = local_time_index + 1\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Caught an exception: \", str(e))\n",
    "    \n",
    "finally:\n",
    "    exp.stop(db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
